{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea941c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfa08b",
   "metadata": {},
   "source": [
    "# Bellman Equation Q Table Application to Number Guessing Game\n",
    "\n",
    "This script implements the Bellman equation to train a Q table and test it.\n",
    "\n",
    "The game I (try to) apply it to is: the numbers 0-4 are shuffled and hidden (i.e. no repeats). As the player, guess numbers (in the range 0-4) until you get first number in the hidden sequence. Then move onto the second number and guess numbers until you get a match. Continue until you've guessed each number in the hidden sequence.\n",
    "\n",
    "This is a dumb game. And obviously the only strategies are to (1) given a current index, don't repeat guesses and (2) don't guess numbers that you got correct earlier in the sequence.\n",
    "\n",
    "As you'll see, my first attempt fails completely. The issue is that the state and observation space are useless -- they are both just your current index in the answer sequence. My initial thought was that one only needs the memory of the previous guesses to \"beat\" this game. Fair enough, but I haven't set this situation up to use that info. Either I need a different algorithm and/or more q-table dimensions OR I need to provide the \"AI\" with a \"memory\".\n",
    "\n",
    "The lesson learned in this first failure is that what you provide in observation/state must be a conscious choice. A thoughtless choice of observation will just probably not be adequate.\n",
    "\n",
    "Recognizing that in real-world applications, you probably won't be throttling the observation space at all. Anything that could be useful (and which doesn't increase your training time too badly) you want to include."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31b28d",
   "metadata": {},
   "source": [
    "### Explore NumberGuess Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be06842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from number_guess_environment import NumberGuess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6aa0203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random valid answer number: 3\n",
      "Random valid guess number: 0\n",
      "\n",
      "A few random games:\n",
      "Episode:1 Score:-10 NGuesses:20\n",
      "Episode:2 Score:-20 NGuesses:30\n",
      "Episode:3 Score:-49 NGuesses:59\n",
      "Episode:4 Score:-15 NGuesses:25\n",
      "Episode:5 Score:-10 NGuesses:20\n",
      "Episode:6 Score:-3 NGuesses:13\n",
      "Episode:7 Score:-7 NGuesses:17\n",
      "Episode:8 Score:-27 NGuesses:37\n",
      "Episode:9 Score:-23 NGuesses:33\n",
      "Episode:10 Score:-9 NGuesses:19\n"
     ]
    }
   ],
   "source": [
    "env = NumberGuess()\n",
    "\n",
    "print(\"Random valid answer number:\", env.observation_space.sample())\n",
    "print(\"Random valid guess number:\", env.action_space.sample())\n",
    "\n",
    "\n",
    "print(\"\\nA few random games:\")\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    n_guesses = 0\n",
    "    while not done:\n",
    "        n_guesses += 1\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score} NGuesses:{n_guesses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7750419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79e79f",
   "metadata": {},
   "source": [
    "### Update and training/testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fde8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_utils import train_test, default_params, update_q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfd31c",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57371824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial q table full of zeroes\n",
    "init_q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f65f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: -15.42\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "env = NumberGuess(False)\n",
    "q_table, avg_reward = train_test(env, init_q_table, n_episodes = 100, do_train = True)\n",
    "print(f\"average reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4944248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-19.52775925 -19.2651705  -19.5375084  -18.72958765 -19.51862032]\n",
      " [-20.52425101 -22.15592013 -22.24453604 -21.41609695 -21.25548207]\n",
      " [-22.43160358 -22.41701668 -22.26308656 -22.16992186 -22.33196317]\n",
      " [-23.25173778 -24.17000836 -24.23712759 -24.25238603 -23.3517368 ]\n",
      " [-26.04345078 -26.07544955 -25.16789394 -26.07537687 -26.16682292]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163ea610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: -35.79\n",
      "average reward: -34.59\n",
      "average reward: -34.25\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "avg_reward = train_test(env, q_table, n_episodes = 100, do_train = False)[1]\n",
    "print(f\"average reward: {avg_reward}\")\n",
    "avg_reward = train_test(env, q_table, n_episodes = 100, do_train = False)[1]\n",
    "print(f\"average reward: {avg_reward}\")\n",
    "avg_reward = train_test(env, q_table, n_episodes = 100, do_train = False)[1]\n",
    "print(f\"average reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbc217",
   "metadata": {},
   "source": [
    "Fail! Of course, in hindsight the naive Bellman equation + q table can't get this right. Because the numbers are random, it only recognizes that no number is a good guess. What it /needs/ is memory of its previous guesses, and knowledge of the previous correct answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56ec4e",
   "metadata": {},
   "source": [
    "## Small improvement\n",
    "In this case, a simple and dumb solution to our problem can be to expand the observation space/state. Brainstorming here...\n",
    "\n",
    "The state could encode:\n",
    "\n",
    "* knowledge of answer sequence, e.g. [2,4,-1,-1,-1], dim: (5+1)! at most\n",
    "* guesses in this iteration: [0,0,1,1,0] or some bit rep? 2*5\n",
    "\n",
    "So this state space dimensionality is: 5! * 10 = 1200\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "OK, not all of this thinking was right, but anyway, I'm in a new notebook now, number_guess2, and a new environment NumberGuess2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
