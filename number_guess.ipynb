{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea941c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfa08b",
   "metadata": {},
   "source": [
    "# Bellman Equation Q Table Application to Number Guessing Game\n",
    "\n",
    "This script implements the Bellman equation to train a Q table and test it.\n",
    "\n",
    "The game I (try to) apply it to is: the numbers 0-4 are shuffled and hidden (i.e. no repeats). As the player, guess numbers (in the range 0-4) until you get first number in the hidden sequence. Then move onto the second number and guess numbers until you get a match. Continue until you've guessed each number in the hidden sequence.\n",
    "\n",
    "This is a dumb game. And obviously the only strategies are to (1) given a current index, don't repeat guesses and (2) don't guess numbers that you got correct earlier in the sequence.\n",
    "\n",
    "As you'll see, my first attempt fails completely. The issue is that the state and observation space are useless -- they are both just your current index in the answer sequence. My initial thought was that one only needs the memory of the previous guesses to \"beat\" this game. Fair enough, but I haven't set this situation up to use that info. Either I need a different algorithm and/or more q-table dimensions OR I need to provide the \"AI\" with a \"memory\".\n",
    "\n",
    "The lesson learned in this first failure is that what you provide in observation/state must be a conscious choice. A thoughtless choice of observation will just probably not be adequate.\n",
    "\n",
    "Recognizing that in real-world applications, you probably won't be throttling the observation space at all. Anything that could be useful (and which doesn't increase your training time too badly) you want to include."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31b28d",
   "metadata": {},
   "source": [
    "### Explore NumberGuess Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be06842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from number_guess import NumberGuess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6aa0203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random valid answer number: 2\n",
      "Random valid guess number: 4\n",
      "\n",
      "A few random games:\n",
      "Episode:1 Score:-17 NGuesses:27\n",
      "Episode:2 Score:-15 NGuesses:25\n",
      "Episode:3 Score:-11 NGuesses:21\n",
      "Episode:4 Score:-18 NGuesses:28\n",
      "Episode:5 Score:-15 NGuesses:25\n",
      "Episode:6 Score:-31 NGuesses:41\n",
      "Episode:7 Score:-10 NGuesses:20\n",
      "Episode:8 Score:-2 NGuesses:12\n",
      "Episode:9 Score:-38 NGuesses:48\n",
      "Episode:10 Score:-5 NGuesses:15\n"
     ]
    }
   ],
   "source": [
    "env = NumberGuess()\n",
    "\n",
    "print(\"Random valid answer number:\", env.observation_space.sample())\n",
    "print(\"Random valid guess number:\", env.action_space.sample())\n",
    "\n",
    "\n",
    "print(\"\\nA few random games:\")\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    n_guesses = 0\n",
    "    while not done:\n",
    "        n_guesses += 1\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score} NGuesses:{n_guesses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7750419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79e79f",
   "metadata": {},
   "source": [
    "### Update and training/testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5fdb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(params, q_table, state_i, state_f, action, step_reward):\n",
    "    try:\n",
    "        # note about 2d np array access:\n",
    "        # q_table[state_i] accesses the state_i-th row, which is an array with\n",
    "        # length equal to number of actions.\n",
    "        # q_table[state_i, action] access the action-th element of the state_ith\n",
    "        # array.\n",
    "        old_q_value = q_table[state_i, action]\n",
    "    except IndexError:\n",
    "        print(\"ERROR with q_table\")\n",
    "        print(q_table)\n",
    "        print(state_i, action)\n",
    "        return None\n",
    "    \n",
    "    # max q value given the state after this temp change\n",
    "    next_max = np.max(q_table[state_f])\n",
    "    q_target = step_reward + params['gamma'] * next_max\n",
    "    q_delta = q_target - old_q_value\n",
    "    q_table[state_i, action] = old_q_value + params['alpha'] * q_delta\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0591afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha - learning rate\n",
    "# gamma - discount rate\n",
    "# epsilon - exploration threshold (not currently used)\n",
    "default_params = {'alpha' : 0.9, 'gamma' : 1., 'epsilon' : 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7673afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop number_guess games\n",
    "def train_test(env, in_q_table, n_episodes = 5, do_train = True, params = default_params):\n",
    "    q_table = in_q_table.copy()\n",
    "    total_reward = 0\n",
    "    for i_game in range(n_episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        state_i = env.state\n",
    "        game_reward = 0\n",
    "        #print(i_game)\n",
    "        while not done:\n",
    "            # choose action\n",
    "            action = env.action_space.sample()\n",
    "            if not do_train and game_reward > -20:\n",
    "                action = np.argmax(q_table[state_i])\n",
    "\n",
    "            # take a step\n",
    "            state_f, reward, done, info = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                continue\n",
    "            \n",
    "            #print(\"  \", env.shower_time, state, reward, done)\n",
    "            try:\n",
    "                assert state_f in env.observation_space\n",
    "            except AssertionError:\n",
    "                print(\"Invalid state obtained\", state_f, done, i_game, action)\n",
    "                break\n",
    "                                        \n",
    "            # update q table\n",
    "            if do_train:\n",
    "                q_table = update_q_table(params, q_table, state_i, state_f, action, reward)\n",
    "\n",
    "            # increment reward\n",
    "            game_reward += reward\n",
    "\n",
    "            state_i = state_f\n",
    "            \n",
    "        #print(\"  Shower reward:\", shower_reward)\n",
    "        total_reward += game_reward\n",
    "\n",
    "    #np.savetxt(\"qtable.csv\", q_table, delimiter=\",\")\n",
    "    avg_reward = total_reward / n_episodes\n",
    "    return q_table, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfd31c",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "57371824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial q table full of zeroes\n",
    "init_q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8f65f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: -16.32\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "env = NumberGuess(False)\n",
    "q_table, avg_reward = train_test(env, init_q_table, n_episodes = 100, do_train = True)\n",
    "print(f\"average reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e4944248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-20.20924731 -20.27151059 -20.27360285 -20.27360884 -20.15682504]\n",
      " [-22.31298974 -22.30130706 -22.30857923 -21.76121376 -22.31309934]\n",
      " [-23.8018177  -23.81098675 -23.81098623 -23.80217989 -23.62497236]\n",
      " [-26.6153246  -25.6154146  -25.81064852 -26.52540479 -26.60641285]\n",
      " [-27.80334063 -26.82235061 -27.82224161 -26.82235062 -27.80235061]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "163ea610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: -33.8\n",
      "average reward: -37.47\n",
      "average reward: -33.46\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "avg_reward = train_test(env, q_table, n_episodes = 100, do_train = False)[1]\n",
    "print(f\"average reward: {avg_reward}\")\n",
    "avg_reward = train_test(env, q_table, n_episodes = 100, do_train = False)[1]\n",
    "print(f\"average reward: {avg_reward}\")\n",
    "avg_reward = train_test(env, q_table, n_episodes = 100, do_train = False)[1]\n",
    "print(f\"average reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbc217",
   "metadata": {},
   "source": [
    "Fail! Of course, in hindsight the naive Bellman equation + q table can't get this right. Because the numbers are random, it only recognizes that no number is a good guess. What it /needs/ is memory of its previous guesses, and knowledge of the previous correct answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56ec4e",
   "metadata": {},
   "source": [
    "## Small improvement\n",
    "In this case, a simple and dumb solution to our problem can be to expand the observation space/state. Brainstorming here...\n",
    "\n",
    "The state could encode:\n",
    "\n",
    "* knowledge of answer sequence, e.g. [2,4,-1,-1,-1], dim: (5+1)! at most\n",
    "* guesses in this iteration: [0,0,1,1,0] or some bit rep? 2*5\n",
    "\n",
    "So this state space dimensionality is: 5! * 10 = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec62f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from number_guess import NumberGuess2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55cb3215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random valid answer number: 64755\n",
      "Random valid guess number: 0\n",
      "\n",
      "A few random games:\n",
      "Episode:1 Score:-4 NGuesses:14\n",
      "Episode:2 Score:-12 NGuesses:22\n",
      "Episode:3 Score:-27 NGuesses:37\n",
      "Episode:4 Score:-6 NGuesses:16\n",
      "Episode:5 Score:-13 NGuesses:23\n",
      "Episode:6 Score:-44 NGuesses:54\n",
      "Episode:7 Score:-30 NGuesses:40\n",
      "Episode:8 Score:-11 NGuesses:21\n",
      "Episode:9 Score:-16 NGuesses:26\n",
      "Episode:10 Score:-12 NGuesses:22\n"
     ]
    }
   ],
   "source": [
    "env = NumberGuess2()\n",
    "\n",
    "print(\"Random valid answer number:\", env.observation_space.sample())\n",
    "print(\"Random valid guess number:\", env.action_space.sample())\n",
    "\n",
    "\n",
    "print(\"\\nA few random games:\")\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    n_guesses = 0\n",
    "    while not done:\n",
    "        n_guesses += 1\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score} NGuesses:{n_guesses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec585e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e818f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab55ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
