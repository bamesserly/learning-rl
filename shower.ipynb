{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75463bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07524bed",
   "metadata": {},
   "source": [
    "# Shower Temperature Beginner Reinforcement Learning Example\n",
    "\n",
    "This script implements the Bellman equation to train a Q table and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shower_environment import Shower\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594397c8",
   "metadata": {},
   "source": [
    "## Sanity checks on our shower environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd8e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random valid temperature: 55\n",
      "Random valid temperature change: 1\n",
      "\n",
      "A few random showers:\n",
      "Episode: 1 Score: -40\n",
      "Episode: 2 Score: -60\n",
      "Episode: 3 Score: -60\n",
      "Episode: 4 Score: -38\n",
      "Episode: 5 Score: 22\n"
     ]
    }
   ],
   "source": [
    "env = Shower()\n",
    "\n",
    "print(\"Random valid temperature:\", env.observation_space.sample())\n",
    "print(\"Random valid temperature change:\", env.action_space.sample()-1)\n",
    "\n",
    "print(\"\\nA few random showers:\")\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f\"Episode: {episode} Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3944b",
   "metadata": {},
   "source": [
    "## Update q table function\n",
    "* Based on initial and final states\n",
    "* the action that took us from initial to final, \n",
    "* and the reward assigned to the action/final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c0d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state_i, state_f, action, step_reward):\n",
    "    alpha = 0.9  # learning rate\n",
    "    gamma = 1.0  # discount rate\n",
    "    epsilon = 0.0  # exploration threshold\n",
    "    try:\n",
    "        # note about 2d np array access:\n",
    "        # q_table[state_i] accesses the state_i-th row, which is an array with\n",
    "        # length equal to number of actions.\n",
    "        # q_table[state_i, action] access the action-th element of the state_ith\n",
    "        # array.\n",
    "        old_q_value = q_table[state_i, action]\n",
    "    except IndexError:\n",
    "        print(\"ERROR with q_table\")\n",
    "        print(q_table)\n",
    "        print(state_i, action)\n",
    "        return None\n",
    "    \n",
    "    # max q value given the state after this temp change\n",
    "    next_max = np.max(q_table[state_f])\n",
    "    q_target = step_reward + gamma * next_max\n",
    "    q_delta = q_target - old_q_value\n",
    "    q_table[state_i, action] = old_q_value + alpha * q_delta\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a9ee0",
   "metadata": {},
   "source": [
    "## Train and test function\n",
    "Loop showers, update q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87fcb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop showers\n",
    "def train_test(env, q_table, episodes = 200, do_train = True):\n",
    "    total_reward = 0\n",
    "    for i_shower in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        state_i = env.state\n",
    "        shower_reward = 0\n",
    "        #print(i_shower)\n",
    "        while not done:\n",
    "            # choose action\n",
    "            action = env.action_space.sample() if do_train else np.argmax(q_table[state_i])\n",
    "\n",
    "            # take a step\n",
    "            state_f, reward, done, info = env.step(action)\n",
    "            #print(\"  \", env.shower_time, state, reward, done)\n",
    "            try:\n",
    "                assert state_f in env.observation_space\n",
    "            except AssertionError:\n",
    "                print(\"Invalid state obtained\", state_f, i_shower, env.shower_time, action)\n",
    "                break\n",
    "                                        \n",
    "            # update q table\n",
    "            if do_train:\n",
    "                q_table = update_q_table(q_table, state_i, state_f, action, reward)\n",
    "\n",
    "            # increment reward\n",
    "            shower_reward += reward\n",
    "\n",
    "            state_i = state_f\n",
    "            \n",
    "        #print(\"  Shower reward:\", shower_reward)\n",
    "        total_reward += shower_reward\n",
    "\n",
    "    #np.savetxt(\"qtable.csv\", q_table, delimiter=\",\")\n",
    "    print(\"average reward:\", total_reward / episodes)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8c83d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efddeaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: -22.78\n"
     ]
    }
   ],
   "source": [
    "env = Shower()\n",
    "init_q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "q_table = train_test(env, init_q_table, episodes = 100, do_train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921baf0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029e1816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 58.96\n",
      "average reward: 59.0\n",
      "average reward: 59.0\n"
     ]
    }
   ],
   "source": [
    "train_test(env, q_table, episodes = 100, do_train = False);\n",
    "train_test(env, q_table, episodes = 100, do_train = False);\n",
    "train_test(env, q_table, episodes = 100, do_train = False);"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/python",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
